{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/citipop/disastertweet?scriptVersionId=134343393\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nimport string\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nimport torch.nn.functional as F\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-21T07:43:58.342833Z","iopub.execute_input":"2023-06-21T07:43:58.343501Z","iopub.status.idle":"2023-06-21T07:44:06.522752Z","shell.execute_reply.started":"2023-06-21T07:43:58.343468Z","shell.execute_reply":"2023-06-21T07:44:06.52113Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\nArchive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Download**","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\nX_train = df_train[\"text\"]\nX_test = df_test[\"text\"]\ny_train = df_train[\"target\"]","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:44:06.525728Z","iopub.execute_input":"2023-06-21T07:44:06.527146Z","iopub.status.idle":"2023-06-21T07:44:06.636938Z","shell.execute_reply.started":"2023-06-21T07:44:06.527097Z","shell.execute_reply":"2023-06-21T07:44:06.635377Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# **Preprocess**","metadata":{}},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\nstw = stopwords.words(\"english\")\npd.options.mode.chained_assignment = None\n\ndef preprocess(X):\n    i = 0\n    for sent in X:\n        sent = sent.lower()\n        sent = word_tokenize(sent)\n        sent = [token for token in sent if token not in string.punctuation]\n        sent = [lemmatizer.lemmatize(token) for token in sent if token not in stw]\n        sent = [token for token in sent if len(token)>2]\n        sent = [token for token in sent if token.isalpha()]\n        X.loc[i] = sent\n        i+=1\n    return X\n\nX_train = preprocess(X_train)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:44:06.638776Z","iopub.execute_input":"2023-06-21T07:44:06.639247Z","iopub.status.idle":"2023-06-21T07:44:16.825966Z","shell.execute_reply.started":"2023-06-21T07:44:06.639213Z","shell.execute_reply":"2023-06-21T07:44:16.824757Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Pick most only common words and convert minor words to \"<unk>\"\nmax_vocab = 1000\nall_tokens = [token for tokens in X_train for token in tokens]\ncommon_tokens = set(list(zip(*Counter(all_tokens).most_common(max_vocab)))[0])\nvocab = set(token for token in common_tokens)\nvocab = [\"<pad>\"] + [\"<unk>\"] + sorted(list(vocab))\nword2id = dict((word, i) for i, word in enumerate(vocab))\nid2word = dict((i, word) for i, word in enumerate(vocab))\nX_train = [[token if token in vocab else \"<unk>\" for token in tokens]for tokens in X_train]\n\n#Padding\nX_train = [[word2id.get(token) for token in sent] for sent in X_train]\nX_train = torch.nn.utils.rnn.pad_sequence([torch.tensor(id_list) for id_list in X_train], batch_first=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:44:16.829129Z","iopub.execute_input":"2023-06-21T07:44:16.829538Z","iopub.status.idle":"2023-06-21T07:44:17.845804Z","shell.execute_reply.started":"2023-06-21T07:44:16.829499Z","shell.execute_reply":"2023-06-21T07:44:17.844473Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"X_test = preprocess(X_test)\nX_test = [[token if token in vocab else \"<unk>\" for token in tokens]for tokens in X_test]\nX_test = [[word2id.get(token) for token in sent] for sent in X_test]\nX_test = torch.nn.utils.rnn.pad_sequence([torch.tensor(id_list) for id_list in X_test], batch_first=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:44:17.847645Z","iopub.execute_input":"2023-06-21T07:44:17.848094Z","iopub.status.idle":"2023-06-21T07:44:21.378929Z","shell.execute_reply.started":"2023-06-21T07:44:17.848054Z","shell.execute_reply":"2023-06-21T07:44:21.37746Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# **Datasets**","metadata":{}},{"cell_type":"code","source":"y_train = list(y_train)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:44:21.380735Z","iopub.execute_input":"2023-06-21T07:44:21.381162Z","iopub.status.idle":"2023-06-21T07:44:21.408036Z","shell.execute_reply.started":"2023-06-21T07:44:21.381127Z","shell.execute_reply":"2023-06-21T07:44:21.406713Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Dataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n        \n    def __len__(self):\n        return len(self.X)\n        \n    def __getitem__(self, idx):\n        X = self.X[idx]\n        y = self.y[idx]\n        return X, y\n\ntrain_set = Dataset(X_train, y_train)\nvalid_set =  Dataset(X_val, y_val)\ntrain_loader = DataLoader(train_set, batch_size=64, shuffle=True)\nvalid_loader = DataLoader(valid_set, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:44:21.410087Z","iopub.execute_input":"2023-06-21T07:44:21.410727Z","iopub.status.idle":"2023-06-21T07:44:21.422657Z","shell.execute_reply.started":"2023-06-21T07:44:21.41068Z","shell.execute_reply":"2023-06-21T07:44:21.421319Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# **Model**","metadata":{}},{"cell_type":"code","source":"class RNNClassifier(nn.Module):\n    def __init__(self, output_size, hidden_size, vocab_size, padding_idx,\n                 device, dropout_probability=0.3, bidirectional=False, n_layers=1,\n                 embedding_dimension=50, batch_size=32):\n        super(RNNClassifier, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.batch_size = batch_size\n        self.n_layers = n_layers\n        self.dropout_probability = dropout_probability\n        self.device = device\n        self.padding_idx = padding_idx\n        \n        # We need to multiply some layers by two if the model is bidirectional\n        self.input_size_factor = 2 if bidirectional else 1\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dimension)\n        \n        self.rnn = nn.LSTM(\n            embedding_dimension,\n            self.hidden_size,\n            self.n_layers,\n            bidirectional=bidirectional,\n        )\n\n        self.fc1 = nn.Linear(\n            self.hidden_size * self.input_size_factor,\n            16,\n        )\n        self.fc2 = nn.Linear(\n            16,\n            self.output_size,\n        )\n\n\n    def init_hidden(self):\n        h0 = torch.randn(\n            self.n_layers * self.input_size_factor,\n            self.batch_size,\n            self.hidden_size,\n        )\n        c0 = torch.randn(\n            self.n_layers * self.input_size_factor,\n            self.batch_size,\n            self.hidden_size,\n        )\n        \n        h0 = h0.to(self.device)\n        c0 = c0.to(self.device)\n\n        return h0, c0\n    \n    def apply_rnn(self, embedding_out, lengths):\n        packed = pack_padded_sequence(\n            embedding_out,\n            lengths,\n            batch_first=True,\n        )\n        activations, _ = self.rnn(packed, self.init_hidden())\n        activations, _ = pad_packed_sequence(activations, batch_first=True)\n        \n        indices = (lengths - 1).view(-1, 1).expand(\n            activations.size(0), activations.size(2),\n        ).unsqueeze(1)\n        indices = indices.to(self.device)\n        \n        activations = activations.gather(1, indices).squeeze(1)\n        return activations\n\n    def forward(self, X, return_activations=False):\n        batch_size = len(X)\n        \n        if batch_size != self.batch_size:\n            self.batch_size = batch_size\n\n        lengths = torch.LongTensor([len(tokens) for tokens in X])\n        lengths, permutation_indices = lengths.sort(0, descending=True)\n        \n        X = torch.LongTensor(X)\n        X = X[permutation_indices].to(self.device)\n        \n        embedding_out = self.embedding(X)\n        \n        activations = self.apply_rnn(embedding_out, lengths)\n\n        x = F.dropout(torch.relu(self.fc1(activations)), 0.05)\n        x = self.fc2(x)\n        out = torch.sigmoid(x)\n\n\n        permutation_index_pairs = list(zip(\n            permutation_indices.tolist(),\n            list(range(len(permutation_indices))),\n        ))\n        reordered_indices = [\n            pair[1] for pair\n            in sorted(permutation_index_pairs, key=lambda pair: pair[0])\n        ]\n\n        if return_activations:\n            return out[reordered_indices], x[reordered_indices]\n\n        return out[reordered_indices]","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:44:21.424141Z","iopub.execute_input":"2023-06-21T07:44:21.425024Z","iopub.status.idle":"2023-06-21T07:44:21.449251Z","shell.execute_reply.started":"2023-06-21T07:44:21.424989Z","shell.execute_reply":"2023-06-21T07:44:21.447509Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"for i, j in train_loader:\n    samp_X = i\n    break\nmodel(samp_X)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:44:21.451317Z","iopub.execute_input":"2023-06-21T07:44:21.451825Z","iopub.status.idle":"2023-06-21T07:44:21.739077Z","shell.execute_reply.started":"2023-06-21T07:44:21.451782Z","shell.execute_reply":"2023-06-21T07:44:21.736553Z"},"trusted":true},"execution_count":10,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m     samp_X \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m(samp_X)\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}]},{"cell_type":"code","source":"dropout_probability = 0.2  \nn_rnn_layers = 1  \nembedding_dimension = 128  \nhidden_size = 64  \nis_bidirectional = True \nmax_epochs = 10  \nlearning_rate = 0.001 \nbatch_size = 64\n\nmodel = RNNClassifier(\n    output_size=2,  \n    hidden_size=hidden_size,\n    embedding_dimension=embedding_dimension,\n    vocab_size=len(vocab),\n    padding_idx=word2id['<pad>'],\n    dropout_probability=dropout_probability,\n    bidirectional=is_bidirectional,\n    n_layers=n_rnn_layers,\n    device=device,\n    batch_size=batch_size,\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:45:01.900048Z","iopub.execute_input":"2023-06-21T07:45:01.900637Z","iopub.status.idle":"2023-06-21T07:45:01.915161Z","shell.execute_reply.started":"2023-06-21T07:45:01.900597Z","shell.execute_reply":"2023-06-21T07:45:01.913778Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# **Train and Evaluation**","metadata":{}},{"cell_type":"code","source":"from torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(\n    filter(lambda p: p.requires_grad, model.parameters()),\n    lr=learning_rate,\n)\nscheduler = CosineAnnealingLR(optimizer, 1)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:45:04.054634Z","iopub.execute_input":"2023-06-21T07:45:04.055126Z","iopub.status.idle":"2023-06-21T07:45:04.064195Z","shell.execute_reply.started":"2023-06-21T07:45:04.055084Z","shell.execute_reply":"2023-06-21T07:45:04.06238Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def train_epoch(model, optimizer, scheduler, train_loader):\n    model.train()\n    total_loss = total = 0\n    progress_bar = tqdm_notebook(train_loader, desc='Training', leave=False)\n    for inputs, target in progress_bar:\n        target = target.to(device)\n\n        # Clean old gradients\n        optimizer.zero_grad()\n\n        # Forwards pass\n        output = model(inputs)\n\n        # Calculate how wrong the model is\n        loss = criterion(output, target)\n\n        # Perform gradient descent, backwards pass\n        loss.backward()\n\n        # Take a step in the right direction\n        optimizer.step()\n        scheduler.step()\n\n        # Record metrics\n        total_loss += loss.item()\n        total += len(target)\n\n    return total_loss / total\n\n\ndef validate_epoch(model, valid_loader):\n    model.eval()\n    total_loss = total = 0\n    with torch.no_grad():\n        progress_bar = tqdm_notebook(valid_loader, desc='Validating', leave=False)\n        for inputs, target in progress_bar:\n            target = target.to(device)\n\n            # Forwards pass\n            output = model(inputs)\n\n            # Calculate how wrong the model is\n            loss = criterion(output, target)\n\n            # Record metrics\n            total_loss += loss.item()\n            total += len(target)\n\n    return total_loss / total","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:45:06.795077Z","iopub.execute_input":"2023-06-21T07:45:06.795553Z","iopub.status.idle":"2023-06-21T07:45:06.807167Z","shell.execute_reply.started":"2023-06-21T07:45:06.795519Z","shell.execute_reply":"2023-06-21T07:45:06.806125Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm, tqdm_notebook\n\nn_epochs = 0\ntrain_losses, valid_losses = [], []\nfor _ in range(max_epochs):\n    train_loss = train_epoch(model, optimizer, scheduler, train_loader)\n    valid_loss = validate_epoch(model, valid_loader)\n\n    tqdm.write(\n        f'epoch #{n_epochs + 1:3d}\\ttrain_loss: {train_loss:.2e}'\n        f'\\tvalid_loss: {valid_loss:.2e}\\n',\n    )\n\n    # Early stopping if the current valid_loss is greater than the last three valid losses\n    if len(valid_losses) > 2 and all(valid_loss >= loss\n                                     for loss in valid_losses[-3:]):\n        print('Stopping early')\n        break\n\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n\n    n_epochs += 1","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:45:09.638174Z","iopub.execute_input":"2023-06-21T07:45:09.638908Z","iopub.status.idle":"2023-06-21T07:45:58.94988Z","shell.execute_reply.started":"2023-06-21T07:45:09.638872Z","shell.execute_reply":"2023-06-21T07:45:58.948351Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_32/561019931.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  progress_bar = tqdm_notebook(train_loader, desc='Training', leave=False)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/108 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_32/561019931.py:35: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  progress_bar = tqdm_notebook(valid_loader, desc='Validating', leave=False)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch #  1\ttrain_loss: 1.08e-02\tvalid_loss: 1.08e-02\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/108 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch #  2\ttrain_loss: 1.06e-02\tvalid_loss: 1.03e-02\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/108 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch #  3\ttrain_loss: 1.00e-02\tvalid_loss: 9.82e-03\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/108 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch #  4\ttrain_loss: 9.42e-03\tvalid_loss: 9.20e-03\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/108 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch #  5\ttrain_loss: 8.94e-03\tvalid_loss: 9.16e-03\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/108 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch #  6\ttrain_loss: 8.43e-03\tvalid_loss: 8.76e-03\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/108 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch #  7\ttrain_loss: 8.38e-03\tvalid_loss: 8.89e-03\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/108 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch #  8\ttrain_loss: 8.04e-03\tvalid_loss: 8.96e-03\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/108 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch #  9\ttrain_loss: 7.87e-03\tvalid_loss: 8.81e-03\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/108 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"epoch # 10\ttrain_loss: 7.68e-03\tvalid_loss: 8.78e-03\n\n","output_type":"stream"}]},{"cell_type":"code","source":"for X, y in valid_loader:\n    pred = torch.argmax(model(X), dim=1)\n    print(torch.eq(pred, y).sum()/torch.numel(pred)*100)\n    break","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:44:21.751276Z","iopub.status.idle":"2023-06-21T07:44:21.751926Z","shell.execute_reply.started":"2023-06-21T07:44:21.751717Z","shell.execute_reply":"2023-06-21T07:44:21.751738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nepoch_ticks = range(1, n_epochs + 1)\nplt.plot(epoch_ticks, train_losses)\nplt.plot(epoch_ticks, valid_losses)\nplt.legend(['Train Loss', 'Valid Loss'])\nplt.title('Losses')\nplt.xlabel('Epoch #')\nplt.ylabel('Loss')\nplt.xticks(epoch_ticks)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:44:21.753069Z","iopub.status.idle":"2023-06-21T07:44:21.753642Z","shell.execute_reply.started":"2023-06-21T07:44:21.753453Z","shell.execute_reply":"2023-06-21T07:44:21.753472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Test and Submission**","metadata":{}},{"cell_type":"code","source":"class Dataset(Dataset):\n    def __init__(self, X):\n        self.X = X\n        \n    def __len__(self):\n        return len(self.X)\n        \n    def __getitem__(self, idx):\n        X = self.X[idx]\n        return X\n\ntest_set = Dataset(X_test)\ntest_loader = DataLoader(test_set, batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:44:21.75465Z","iopub.status.idle":"2023-06-21T07:44:21.755232Z","shell.execute_reply.started":"2023-06-21T07:44:21.755046Z","shell.execute_reply":"2023-06-21T07:44:21.755064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(model, test_loader, preds_list):\n    with torch.no_grad():\n        for X in test_loader:\n            preds = model(X)\n            preds = preds.argmax(dim=-1)\n            preds_list = preds_list + list(preds)\n        return preds_list\n            \npreds_list = []\npreds_list = test(model, test_loader, preds_list)\npreds_list = list(map(lambda x: int(x), preds_list))","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:44:21.756241Z","iopub.status.idle":"2023-06-21T07:44:21.756795Z","shell.execute_reply.started":"2023-06-21T07:44:21.756611Z","shell.execute_reply":"2023-06-21T07:44:21.75663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\ndf_sub['target'] = preds_list\ndf_sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:44:21.757766Z","iopub.status.idle":"2023-06-21T07:44:21.758308Z","shell.execute_reply.started":"2023-06-21T07:44:21.758123Z","shell.execute_reply":"2023-06-21T07:44:21.758141Z"},"trusted":true},"execution_count":null,"outputs":[]}]}